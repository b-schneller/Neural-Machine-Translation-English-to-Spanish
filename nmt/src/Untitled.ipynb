{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in ENG and SPAN data\n",
    "\n",
    "def read_list_of_sentences(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        content = [line.strip('\\n') for line in content]\n",
    "    return content\n",
    "\n",
    "eng_list = read_list_of_sentences('../data/raw/europarl-v7.es-en.en')\n",
    "span_list = read_list_of_sentences('../data/raw/europarl-v7.es-en.es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_sentence_lists(source_list, target_list, max_len=64):\n",
    "    source_clean, target_clean = list(), list()\n",
    "    punctuation_translator = str.maketrans('','',string.punctuation)\n",
    "    punctuation_translator[191] = None  # to remove inverted question mark\n",
    "    for source, target in zip(source_list, target_list):\n",
    "        if len(source.split()) < (max_len-1) and len(target.split()) < (max_len-1):\n",
    "            if source is not '' and target is not '':\n",
    "                source = source.translate(punctuation_translator)\n",
    "                source = source.replace(\" s \", \"'s \")\n",
    "                target = target.translate(punctuation_translator)\n",
    "                target = target.replace(\" s \", \"'s \")\n",
    "    #             source = pad_to_sequence_length(source, max_len)\n",
    "    #             target = pad_to_sequence_length(target, max_len)\n",
    "                source_clean.append(source.lower())\n",
    "                target_clean.append(target.lower())\n",
    "    return source_clean, target_clean\n",
    "\n",
    "# def pad_source_sequence_length(sentence, max_len):\n",
    "#     padding_length = target_length - len(sentence.split())\n",
    "#     sentence = sentence + ' <PAD>' * padding_length\n",
    "#     sentence = ' '.join(sentence.split()[::-1])\n",
    "#     return sentence\n",
    "    \n",
    "# def pad_target_sequence_length(sentence, target_length):\n",
    "#     padding_length = target_length - len(sentence.split())\n",
    "#     sentence = sentence ' <PAD>' * padding_length\n",
    "#     return sentence\n",
    "\n",
    "eng_list, span_list = clean_sentence_lists(eng_list, span_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build vocabularies for english and spanish\n",
    "def build_vocabulary(sentence_list, vocabulary_size=50000):\n",
    "    tokens = [('<UNK>', None), ('<PAD>', None), ('<EOS>', None), ('<GO>', None)]\n",
    "    vocabulary_size = vocabulary_size - len(tokens)\n",
    "    word_list = [word for line in sentence_list for word in line.split()]\n",
    "    vocabulary = tokens + Counter(word_list).most_common(vocabulary_size)\n",
    "    vocabulary = np.array([word for word, _ in vocabulary])\n",
    "    dictionary = {word: code for code, word in enumerate(vocabulary)}\n",
    "    return dictionary, vocabulary     \n",
    "\n",
    "eng_dictionary, eng_vocabulary = build_vocabulary(eng_list)\n",
    "span_dictionary, span_vocabulary = build_vocabulary(span_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket_dict(eng_sentences, span_sentences):\n",
    "    # returns dict{10: indices, 20: indices ...}\n",
    "    sample_bucket_sizes = []\n",
    "    bucket_dict = {}\n",
    "    for eng_sentence, span_sentence in zip(eng_sentences, span_sentences):\n",
    "        max_len = max(len(eng_sentence.split()), len(span_sentence.split()))\n",
    "        rounded_max_len = roundup(max_len)\n",
    "        sample_bucket_sizes.append(rounded_max_len)\n",
    "    for i in range(10,max(sample_bucket_sizes)+1, 10):\n",
    "        bucket_dict[i] = create_buckets(sample_bucket_sizes, i)\n",
    "        \n",
    "    return bucket_dict\n",
    "\n",
    "def roundup(x):\n",
    "    return int(math.ceil((x+1) / 10.0)) * 10 # x+1 to push *0 into next bucket to account for tokens     \n",
    "\n",
    "def create_buckets(buckets, bucket_len):\n",
    "    return [index for index, value in enumerate(buckets) if value == bucket_len]\n",
    " \n",
    "bucket_dict = create_bucket_dict(eng_list, span_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding and tokens\n",
    "def add_tokens_to_text(source_list, target_list, bucket_dict):\n",
    "    number_of_samples = len(source_list)\n",
    "    source_final, target_input_final, target_output_final = [None] * number_of_samples, [None] * number_of_samples, [None] * number_of_samples\n",
    "    inverse_bucket_dict = invert(bucket_dict)\n",
    "    for index, bucket_size in inverse_bucket_dict.items():\n",
    "        source_final[index] = pad_source_sentences(source_list[index], bucket_size)\n",
    "        target_input_final[index] = pad_target_input_sentences(target_list[index], bucket_size)\n",
    "        target_output_final[index] = pad_target_output_sentences(target_list[index], bucket_size)\n",
    "    source_final_numerical = convert_words_to_numerical_id(source_final, eng_dictionary)\n",
    "    target_input_final_numerical = convert_words_to_numerical_id(target_input_final, span_dictionary)\n",
    "    target_output_final_numerical = convert_words_to_numerical_id(target_output_final, span_dictionary)\n",
    "    \n",
    "    return source_final_numerical, target_input_final_numerical, target_output_final_numerical\n",
    "\n",
    "def pad_source_sentences(sentence, bucket_size):\n",
    "    sentence_length = len(sentence.split())\n",
    "    pad_length = bucket_size - sentence_length\n",
    "    return sentence + ' <PAD>' * pad_length\n",
    "\n",
    "def pad_target_input_sentences(sentence, bucket_size):\n",
    "    sentence_length = len(sentence.split())\n",
    "    pad_length = bucket_size - sentence_length - 1\n",
    "    return '<GO> ' + sentence + ' <PAD>' * pad_length\n",
    "\n",
    "def pad_target_output_sentences(sentence, bucket_size):\n",
    "    sentence_length = len(sentence.split())\n",
    "    pad_length = bucket_size - sentence_length - 1\n",
    "    return sentence + ' <EOS> ' + ' <PAD>' * pad_length\n",
    "    \n",
    "\n",
    "def invert(d):\n",
    "    return dict( (v,k) for k in d for v in d[k] )\n",
    "\n",
    "\n",
    "def convert_words_to_numerical_id(sentence_list, dictionary):\n",
    "    out = []\n",
    "    for sentence in sentence_list:\n",
    "        out.append([dictionary[word] if word in dictionary else dictionary['<UNK>'] for word in sentence.split()])\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "X_in, y_in, y_out = add_tokens_to_text(eng_list, span_list, bucket_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch Generator\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window, data):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "# Input data.\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "vocabulary_size = 50000\n",
    "embedding_size = 150\n",
    "\n",
    "# Look up embeddings for inputs.\n",
    "init_embeds = tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "embeddings = tf.Variable(init_embeds)\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "# Construct the variables for the NCE loss\n",
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                        stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Compute the average NCE loss for the batch.\n",
    "# tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "# time we evaluate the loss.\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(nce_weights, nce_biases, train_labels, embed,\n",
    "                   num_sampled, vocabulary_size))\n",
    "\n",
    "# Construct the Adam optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), axis=1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "# Add variable initializer.\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_steps = 50001\n",
    "# data_index=0\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "\n",
    "#     average_loss = 0\n",
    "#     for step in range(num_steps):\n",
    "#         print(\"\\rIteration: {}\".format(step), end=\"\\t\")\n",
    "#         batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window, eng_numerical_id)\n",
    "#         feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n",
    "\n",
    "#         # We perform one update step by evaluating the training op (including it\n",
    "#         # in the list of returned values for session.run()\n",
    "#         _, loss_val = sess.run([training_op, loss], feed_dict=feed_dict)\n",
    "#         average_loss += loss_val\n",
    "\n",
    "#         if step % 2000 == 0:\n",
    "#             if step > 0:\n",
    "#                 average_loss /= 2000\n",
    "#             # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "#             print(\"Average loss at step \", step, \": \", average_loss)\n",
    "#             average_loss = 0\n",
    "\n",
    "#         # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "#         if step % 10000 == 0:\n",
    "#             sim = similarity.eval()\n",
    "#             for i in range(valid_size):\n",
    "#                 valid_word = eng_vocabulary[valid_examples[i]]\n",
    "#                 top_k = 8 # number of nearest neighbors\n",
    "#                 nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "#                 log_str = \"Nearest to %s:\" % valid_word\n",
    "#                 for k in range(top_k):\n",
    "#                     close_word = eng_vocabulary[nearest[k]]\n",
    "#                     log_str = \"%s %s,\" % (log_str, close_word)\n",
    "#                 print(log_str)\n",
    "\n",
    "#     eng_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_steps = 50001\n",
    "# data_index=0\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "\n",
    "#     average_loss = 0\n",
    "#     for step in range(num_steps):\n",
    "#         print(\"\\rIteration: {}\".format(step), end=\"\\t\")\n",
    "#         batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window, span_numerical_id)\n",
    "#         feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n",
    "\n",
    "#         # We perform one update step by evaluating the training op (including it\n",
    "#         # in the list of returned values for session.run()\n",
    "#         _, loss_val = sess.run([training_op, loss], feed_dict=feed_dict)\n",
    "#         average_loss += loss_val\n",
    "\n",
    "#         if step % 2000 == 0:\n",
    "#             if step > 0:\n",
    "#                 average_loss /= 2000\n",
    "#             # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "#             print(\"Average loss at step \", step, \": \", average_loss)\n",
    "#             average_loss = 0\n",
    "\n",
    "#         # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "#         if step % 10000 == 0:\n",
    "#             sim = similarity.eval()\n",
    "#             for i in range(valid_size):\n",
    "#                 valid_word = span_vocabulary[valid_examples[i]]\n",
    "#                 top_k = 8 # number of nearest neighbors\n",
    "#                 nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "#                 log_str = \"Nearest to %s:\" % valid_word\n",
    "#                 for k in range(top_k):\n",
    "#                     close_word = span_vocabulary[nearest[k]]\n",
    "#                     log_str = \"%s %s,\" % (log_str, close_word)\n",
    "#                 print(log_str)\n",
    "\n",
    "#     span_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('eng_embeddings', eng_embeddings)\n",
    "# np.save('span_embeddings', span_embeddings)\n",
    "eng_embeddings = np.load('eng_embeddings.npy')\n",
    "span_embeddings = np.load('eng_embeddings.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def plot_with_labels(low_dim_embs, labels):\n",
    "#     assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "#     plt.figure(figsize=(18, 18))  #in inches\n",
    "#     for i, label in enumerate(labels):\n",
    "#         x, y = low_dim_embs[i,:]\n",
    "#         plt.scatter(x, y)\n",
    "#         plt.annotate(label,\n",
    "#                      xy=(x, y),\n",
    "#                      xytext=(5, 2),\n",
    "#                      textcoords='offset points',\n",
    "#                      ha='right',\n",
    "#                      va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "# plot_only = 500\n",
    "# low_dim_embs = tsne.fit_transform(span_embeddings[:plot_only,:])\n",
    "# labels = [span_vocabulary[i] for i in range(plot_only)]\n",
    "# plot_with_labels(low_dim_embs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nmt batch generator goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_batch(indices, iteration, batch_size, x_emb=eng_embeddings,\n",
    "                   y_emb=span_embeddings, eng_dict=eng_dictionary, span_dict=span_dictionary,\n",
    "                  eng_sentence_list=eng_list, span_sentence_list=span_list):\n",
    "    \n",
    "    go = span_dict['<GO>']   # int index in vocab\n",
    "    eos = span_dict['<EOS>']\n",
    "    \n",
    "    # indices into sentence lists\n",
    "#     extract_indices = list(range(iteration*batch_size,(iteration+1)*batch_size))\n",
    "    extract_indices = indices[iteration]\n",
    "    \n",
    "    # single english string\n",
    "    eng_sent = eng_sentence_list[extract_indices]\n",
    "    # single spanish string\n",
    "    span_sent = span_sentence_list[extract_indices]\n",
    "    \n",
    "    # vocabulary vector of words in sentences\n",
    "    eng_word_vector = get_indices_from_words(eng_sent, eng_dict)\n",
    "    span_word_vector_start = [go] + get_indices_from_words(span_sent, span_dict)\n",
    "    span_word_vector_end = get_indices_from_words(span_sent, span_dict) + [eos]\n",
    "    \n",
    "    # \n",
    "    X_batch = np.flip(np.array([x_emb[i,:] for i in eng_word_vector]), axis=0)  #good\n",
    "    y_input_batch = np.asarray([y_emb[i,:] for i in span_word_vector_start])\n",
    "    y_target_batch = np.asarray([y_emb[i,:] for i in span_word_vector_end])\n",
    "    \n",
    "    y_one_hot = np.zeros((50000, len(span_sent.split())+1), dtype=np.float32) \n",
    "    \n",
    "    for i in range(len(span_sent.split())):\n",
    "        y_one_hot[span_word_vector_end[i],i] = 1\n",
    "        \n",
    "    y_one_hot[go,-1] = 1\n",
    "    y_one_hot = np.array((y_one_hot))\n",
    "    \n",
    "    sequence_length = len(eng_sent.split())\n",
    "    \n",
    "    return X_batch, y_input_batch, y_target_batch, y_one_hot, sequence_length\n",
    "\n",
    "def get_indices_from_words(sentence, dictionary):\n",
    "    indices = list()\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            indices.append(dictionary[word])\n",
    "        except:\n",
    "            indices.append(dictionary['<UNK>'])\n",
    "    return indices\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMT graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = 100   # sequence length\n",
    "n_inputs = 150     # embedding vector length\n",
    "n_neurons = 128    # whatever\n",
    "vocab_size = 50000 # vocab size / length of one-hot vector\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, None, n_inputs])              # \n",
    "y_input = tf.placeholder(tf.float32, [None, None, n_inputs])\n",
    "y_target = tf.placeholder(tf.float32, [None, None, n_inputs])\n",
    "y_one_hot = tf.placeholder(tf.float32, [None, vocab_size])\n",
    "seq_length = tf.placeholder(tf.int32)\n",
    "target_weights = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons)\n",
    "\n",
    "# Encoder\n",
    "_, state = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32, sequence_length=seq_length)\n",
    "\n",
    "# Decoder\n",
    "output_cell = tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.LSTMCell(num_units=n_neurons),\n",
    "                                                     output_size=vocab_size)\n",
    "\n",
    "outputs, _ = tf.nn.dynamic_rnn(output_cell, y_input, dtype=tf.float32)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=y_one_hot))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0 Loss:  10.811845\n",
      "Iteration:  100 Loss:  7.6193295\n",
      "Iteration:  200 Loss:  6.7295575\n",
      "Iteration:  300 Loss:  8.400658\n",
      "Iteration:  400 Loss:  6.26856\n",
      "Iteration:  500 Loss:  6.404021\n",
      "Iteration:  600 Loss:  7.0464344\n",
      "Iteration:  700 Loss:  7.4069743\n",
      "Iteration:  800 Loss:  6.8112836\n",
      "Iteration:  900 Loss:  7.7679157\n",
      "Iteration:  1000 Loss:  7.232817\n",
      "Iteration:  1100 Loss:  7.0093536\n",
      "Iteration:  1200 Loss:  6.062546\n",
      "Iteration:  1300 Loss:  6.7613754\n",
      "Iteration:  1400 Loss:  5.801794\n",
      "Iteration:  1500 Loss:  6.5890703\n",
      "Iteration:  1600 Loss:  5.762847\n",
      "Iteration:  1700 Loss:  8.57424\n",
      "Iteration:  1800 Loss:  5.79988\n",
      "Iteration:  1900 Loss:  7.008194\n",
      "Iteration:  2000 Loss:  7.649724\n",
      "Iteration:  2100 Loss:  6.621567\n",
      "Iteration:  2200 Loss:  5.9604545\n",
      "Iteration:  2300 Loss:  6.380586\n",
      "Iteration:  2400 Loss:  6.788675\n",
      "Iteration:  2500 Loss:  6.6475773\n",
      "Iteration:  2600 Loss:  7.1200776\n",
      "Iteration:  2700 Loss:  6.5716634\n",
      "Iteration:  2800 Loss:  6.4589496\n",
      "Iteration:  2900 Loss:  6.2228885\n",
      "Iteration:  3000 Loss:  7.767694\n",
      "Iteration:  3100 Loss:  6.696828\n",
      "Iteration:  3200 Loss:  7.501068\n",
      "Iteration:  3300 Loss:  4.367523\n",
      "Iteration:  3400 Loss:  7.458652\n",
      "Iteration:  3500 Loss:  6.5049567\n",
      "Iteration:  3600 Loss:  6.4635973\n",
      "Iteration:  3700 Loss:  6.263728\n",
      "Iteration:  3800 Loss:  5.9637494\n",
      "Iteration:  3900 Loss:  7.0363965\n",
      "Iteration:  4000 Loss:  6.456978\n",
      "Iteration:  4100 Loss:  6.324136\n",
      "Iteration:  4200 Loss:  7.213954\n",
      "Iteration:  4300 Loss:  6.45223\n",
      "Iteration:  4400 Loss:  7.079086\n",
      "Iteration:  4500 Loss:  6.009553\n",
      "Iteration:  4600 Loss:  7.435245\n",
      "Iteration:  4700 Loss:  7.460979\n",
      "Iteration:  4800 Loss:  6.581876\n",
      "Iteration:  4900 Loss:  7.5659447\n",
      "Iteration:  5000 Loss:  6.3510528\n",
      "Iteration:  5100 Loss:  7.3069057\n",
      "Iteration:  5200 Loss:  6.920624\n",
      "Iteration:  5300 Loss:  6.9783535\n",
      "Iteration:  5400 Loss:  6.1260014\n",
      "Iteration:  5500 Loss:  5.851996\n",
      "Iteration:  5600 Loss:  5.951609\n",
      "Iteration:  5700 Loss:  8.034112\n",
      "Iteration:  5800 Loss:  5.9470153\n",
      "Iteration:  5900 Loss:  6.4451795\n",
      "Iteration:  6000 Loss:  6.7236047\n",
      "Iteration:  6100 Loss:  6.0339274\n",
      "Iteration:  6200 Loss:  7.6194\n",
      "Iteration:  6300 Loss:  7.0362935\n",
      "Iteration:  6400 Loss:  5.509565\n",
      "Iteration:  6500 Loss:  6.2345777\n",
      "Iteration:  6600 Loss:  5.7553267\n",
      "Iteration:  6700 Loss:  6.44274\n",
      "Iteration:  6800 Loss:  7.3381157\n",
      "Iteration:  6900 Loss:  8.192254\n",
      "Iteration:  7000 Loss:  6.427666\n",
      "Iteration:  7100 Loss:  7.246658\n",
      "Iteration:  7200 Loss:  6.928437\n",
      "Iteration:  7300 Loss:  7.396118\n",
      "Iteration:  7400 Loss:  6.863698\n",
      "Iteration:  7500 Loss:  6.0839934\n",
      "Iteration:  7600 Loss:  7.757018\n",
      "Iteration:  7700 Loss:  5.3617663\n",
      "Iteration:  7800 Loss:  7.1002765\n",
      "Iteration:  7900 Loss:  7.2014294\n",
      "Iteration:  8000 Loss:  7.064744\n",
      "Iteration:  8100 Loss:  6.8221693\n",
      "Iteration:  8200 Loss:  6.209046\n",
      "Iteration:  8300 Loss:  6.1306515\n",
      "Iteration:  8400 Loss:  6.102562\n",
      "Iteration:  8500 Loss:  6.5646725\n",
      "Iteration:  8600 Loss:  5.8290944\n",
      "Iteration:  8700 Loss:  6.4544735\n",
      "Iteration:  8800 Loss:  5.986576\n",
      "Iteration:  8900 Loss:  6.493486\n",
      "Iteration:  9000 Loss:  5.729754\n",
      "Iteration:  9100 Loss:  5.6829553\n",
      "Iteration:  9200 Loss:  6.85223\n",
      "Iteration:  9300 Loss:  6.779215\n",
      "Iteration:  9400 Loss:  5.6356425\n",
      "Iteration:  9500 Loss:  4.593807\n",
      "Iteration:  9600 Loss:  6.020338\n",
      "Iteration:  9700 Loss:  6.0666347\n",
      "Iteration:  9800 Loss:  6.664645\n",
      "Iteration:  9900 Loss:  5.938255\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "n_iterations = 10000 #len(eng_list) // batch_size\n",
    "n_epochs = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_indices = np.random.permutation(len(eng_list))\n",
    "        for iteration in range(n_iterations):\n",
    "            try:\n",
    "                X_batch, y_input_batch, y_target_batch, y_target_one_hot, sequence_length = get_next_batch(shuffled_indices,\n",
    "                                                                                     iteration, batch_size)\n",
    "                feed_dict = {X: X_batch[np.newaxis, :, :], \n",
    "                             y_input: y_input_batch[np.newaxis, :, :], \n",
    "                             y_target: y_target_batch[np.newaxis, :, :],\n",
    "                             y_one_hot: np.transpose(y_target_one_hot),\n",
    "                             seq_length: sequence_length}\n",
    "                sess.run(training_op, feed_dict=feed_dict)\n",
    "            except:\n",
    "                pass\n",
    "            if iteration % 100 == 0:        \n",
    "                mse = loss.eval(feed_dict=feed_dict)\n",
    "                print('Iteration: ', iteration, 'Loss: ', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
